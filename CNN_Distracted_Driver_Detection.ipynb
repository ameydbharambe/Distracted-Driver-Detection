{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "uofnvxpz-qdI"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiM6gYg0nhkY"
      },
      "source": [
        "<font color=\"#de3023\"><h1><b>REMINDER MAKE A COPY OF THIS NOTEBOOK, DO NOT EDIT</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-u8iBb0fCIgO"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "<font color=\"#d3d3d3\"><h1><b>Reminder for all students and instructors to use the same version of the project notebooks (beginner or advanced)</b></h1></font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T4sLmjIuDpmY"
      },
      "source": [
        "![](http://static1.squarespace.com/static/56ccc8724c2f8548059fbcfe/58f6daea15d5dbcc64ef63aa/5cae2d770d92977242838baa/1557942174418/SW-DistractedDriving-Clean-1.jpg?format=1500w)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T13XTOeamF7y"
      },
      "source": [
        "In this notebook we will be:\n",
        "1.   Understanding and Building Neural Networks\n",
        "2.   Applying Neural Networks to Detecting Distracted Drivers\n",
        "3.   Exploring Convolutional Neural Networks\n",
        "4.   Implementing Transfer Learning\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aSeClkWgIORK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03926d75-73c1-4b6a-a261-bbd630957ef9"
      },
      "source": [
        "#@title Run this to download data and prepare our environment! { display-mode: \"form\" }\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import tensorflow.keras\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, MaxPooling2D, Dropout, Flatten, Reshape, Dense, Conv2D, GlobalAveragePooling2D\n",
        "import tensorflow.keras.optimizers as optimizers\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.applications import VGG16, VGG19, ResNet50, DenseNet121\n",
        "\n",
        "\n",
        "def label_to_numpy(labels):\n",
        "  final_labels = np.zeros((len(labels), 4))\n",
        "  for i in range(len(labels)):\n",
        "    label = labels[i]\n",
        "    if label == 'Attentive':\n",
        "      final_labels[i,:] = np.array([1, 0, 0, 0])\n",
        "    if label == 'DrinkingCoffee':\n",
        "      final_labels[i,:] = np.array([0, 1, 0, 0])\n",
        "    if label == 'UsingMirror':\n",
        "      final_labels[i,:] = np.array([0, 0, 1, 0])\n",
        "    if label == 'UsingRadio':\n",
        "      final_labels[i,:] = np.array([0, 0, 0, 1])\n",
        "  return final_labels\n",
        "\n",
        "class pkg:\n",
        "  #### DOWNLOADING AND LOADING DATA\n",
        "  def get_metadata(metadata_path, which_splits = ['train', 'test']):\n",
        "    '''returns metadata dataframe which contains columns of:\n",
        "       * index: index of data into numpy data\n",
        "       * class: class of image\n",
        "       * split: which dataset split is this a part of?\n",
        "    '''\n",
        "    metadata = pd.read_csv(metadata_path)\n",
        "    keep_idx = metadata['split'].isin(which_splits)\n",
        "    metadata = metadata[keep_idx]\n",
        "\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    return metadata\n",
        "\n",
        "  def get_data_split(split_name, flatten, all_data, metadata, image_shape):\n",
        "    '''\n",
        "    returns images (data), labels from folder of format [image_folder]/[split_name]/[class_name]/\n",
        "    flattens if flatten option is True\n",
        "    '''\n",
        "    # Get dataframes for each class.\n",
        "    df_coffee_train = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_coffee_test = metadata[(metadata['class'] == 'DrinkingCoffee') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_mirror_train = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_mirror_test = metadata[(metadata['class'] == 'UsingMirror') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_attentive_train = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_attentive_test = metadata[(metadata['class'] == 'Attentive') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "    df_radio_train = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'train')]\n",
        "    df_radio_test = metadata[(metadata['class'] == 'UsingRadio') & \\\n",
        "                         (metadata['split'] == 'test')]\n",
        "\n",
        "    # Get number of items in class with lowest number of images.\n",
        "    num_samples_train = min(df_coffee_train.shape[0], \\\n",
        "                            df_mirror_train.shape[0], \\\n",
        "                            df_attentive_train.shape[0], \\\n",
        "                            df_radio_train.shape[0])\n",
        "    num_samples_test = min(df_coffee_test.shape[0], \\\n",
        "                            df_mirror_test.shape[0], \\\n",
        "                            df_attentive_test.shape[0], \\\n",
        "                            df_radio_test.shape[0])\n",
        "\n",
        "    # Resample each of the classes and concatenate the images.\n",
        "    metadata_train = pd.concat([df_coffee_train.sample(num_samples_train), \\\n",
        "                          df_mirror_train.sample(num_samples_train), \\\n",
        "                          df_attentive_train.sample(num_samples_train), \\\n",
        "                          df_radio_train.sample(num_samples_train) ])\n",
        "    metadata_test = pd.concat([df_coffee_test.sample(num_samples_test), \\\n",
        "                          df_mirror_test.sample(num_samples_test), \\\n",
        "                          df_attentive_test.sample(num_samples_test), \\\n",
        "                          df_radio_test.sample(num_samples_test) ])\n",
        "\n",
        "    metadata = pd.concat( [metadata_train, metadata_test] )\n",
        "\n",
        "    sub_df = metadata[metadata['split'].isin([split_name])]\n",
        "    index  = sub_df['index'].values\n",
        "    labels = sub_df['class'].values\n",
        "    data = all_data[index,:]\n",
        "    if flatten:\n",
        "      data = data.reshape([-1, np.product(image_shape)])\n",
        "    return data, labels\n",
        "\n",
        "  def get_train_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('train', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_test_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('test', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "  def get_field_data(flatten, all_data, metadata, image_shape):\n",
        "    return get_data_split('field', flatten, all_data, metadata, image_shape)\n",
        "\n",
        "class helpers:\n",
        "  #### PLOTTING\n",
        "  def plot_image(data, num_ims, figsize=(8,6), labels = [], index = None, image_shape = [64,64,3]):\n",
        "    '''\n",
        "    if data is a single image, display that image\n",
        "\n",
        "    if data is a 4d stack of images, display that image\n",
        "    '''\n",
        "    print(data.shape)\n",
        "    num_dims   = len(data.shape)\n",
        "    num_labels = len(labels)\n",
        "\n",
        "    # reshape data if necessary\n",
        "    if num_dims == 1:\n",
        "      data = data.reshape(target_shape)\n",
        "    if num_dims == 2:\n",
        "      data = data.reshape(-1,image_shape[0],image_shape[1],image_shape[2])\n",
        "    num_dims   = len(data.shape)\n",
        "\n",
        "    # check if single or multiple images\n",
        "    if num_dims == 3:\n",
        "      if num_labels > 1:\n",
        "        print('Multiple labels does not make sense for single image.')\n",
        "        return\n",
        "\n",
        "      label = labels\n",
        "      if num_labels == 0:\n",
        "        label = ''\n",
        "      image = data\n",
        "\n",
        "    if num_dims == 4:\n",
        "      image = data[index, :]\n",
        "      label = labels[index]\n",
        "\n",
        "    # plot image of interest\n",
        "\n",
        "    nrows=int(np.sqrt(num_ims))\n",
        "    ncols=int(np.ceil(num_ims/nrows))\n",
        "    print(nrows,ncols)\n",
        "    count=0\n",
        "    if nrows==1 and ncols==1:\n",
        "      print('Label: %s'%label)\n",
        "      plt.imshow(image)\n",
        "      plt.show()\n",
        "    else:\n",
        "      print(labels)\n",
        "      fig = plt.figure(figsize=figsize)\n",
        "      for i in range(nrows):\n",
        "        for j in range(ncols):\n",
        "          if count<num_ims:\n",
        "            fig.add_subplot(nrows,ncols,count+1)\n",
        "            plt.imshow(image[count])\n",
        "            count+=1\n",
        "      fig.set_size_inches(18.5, 10.5)\n",
        "      plt.show()\n",
        "\n",
        "  # Checking correct model specs\n",
        "  def model_to_string(model):\n",
        "    import re\n",
        "    stringlist = []\n",
        "    model.summary(print_fn=lambda x: stringlist.append(x))\n",
        "\n",
        "    # Process each line in the summary\n",
        "    standardized_stringlist = []\n",
        "    for line in stringlist:\n",
        "        # Standardize the Dense layer representation\n",
        "        line = re.sub(r'Dense\\((\\d+),', r'Dense(units=\\1,', line)\n",
        "        standardized_stringlist.append(line)\n",
        "\n",
        "    # Combine the processed lines into a single string\n",
        "    sms = \"\\n\".join(standardized_stringlist)\n",
        "\n",
        "    # Remove layer numbering that keras adds\n",
        "    sms = re.sub('_\\d\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d\\d','', sms)\n",
        "    sms = re.sub('_\\d','', sms)\n",
        "    sms = sms.replace(\" \", \"\")\n",
        "\n",
        "    return sms\n",
        "\n",
        "  # plotting model performance\n",
        "  def plot_acc(history, ax = None, xlabel = 'Epoch #'):\n",
        "    history = history.history\n",
        "    history.update({'epoch':list(range(len(history['val_accuracy'])))})\n",
        "    history = pd.DataFrame.from_dict(history)\n",
        "\n",
        "    best_epoch = history.sort_values(by = 'val_accuracy', ascending = False).iloc[0]['epoch']\n",
        "\n",
        "    if not ax:\n",
        "      f, ax = plt.subplots(1,1)\n",
        "    sns.lineplot(x = 'epoch', y = 'val_accuracy', data = history, label = 'Validation', ax = ax)\n",
        "    sns.lineplot(x = 'epoch', y = 'accuracy', data = history, label = 'Training', ax = ax)\n",
        "    ax.axhline(0.25, linestyle = '--',color='red', label = 'Chance')\n",
        "    ax.axvline(x = best_epoch, linestyle = '--', color = 'green', label = 'Best Epoch')\n",
        "    ax.legend(loc = 7)\n",
        "    ax.set_ylim([0.01, 1])\n",
        "\n",
        "    ax.set_xlabel(xlabel)\n",
        "    ax.set_ylabel('Accuracy (Fraction)')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "class models:\n",
        "  def TransferClassifier(name, nn_params):\n",
        "    expert_dict = {'VGG16': VGG16,\n",
        "                   'VGG19': VGG19,\n",
        "                   'ResNet50':ResNet50,\n",
        "                   'DenseNet121':DenseNet121}\n",
        "\n",
        "    expert_conv = expert_dict[name](weights = 'imagenet',\n",
        "                                              include_top = False,\n",
        "                                              input_shape = nn_params['input_shape'])\n",
        "\n",
        "    expert_model = Sequential()\n",
        "    expert_model.add(expert_conv)\n",
        "    expert_model.add(GlobalAveragePooling2D())\n",
        "\n",
        "    expert_model.add(Dense(1024, activation = 'relu'))\n",
        "    expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(512, activation = 'relu'))\n",
        "    expert_model.add(Dropout(0.3))\n",
        "\n",
        "    expert_model.add(Dense(nn_params['output_neurons'],\n",
        "                           activation = nn_params['output_activation']))\n",
        "\n",
        "    expert_model.compile(loss = nn_params['loss'],\n",
        "                  optimizer = optimizers.SGD(learning_rate=1e-10, momentum=0.95),\n",
        "                  metrics=['accuracy'])\n",
        "\n",
        "    return expert_model\n",
        "\n",
        "### defining project variables\n",
        "# file variables\n",
        "# image_data_url       = 'https://drive.google.com/uc?id=1qmTuUyn0525-612yS-wkp8gHB72Wv_XP'\n",
        "# metadata_url         = 'https://drive.google.com/uc?id=1OfKnq3uIT29sXjWSZqOOpceig8Ul24OW'\n",
        "image_data_path      = './image_data.npy'\n",
        "metadata_path        = './metadata.csv'\n",
        "image_shape          = (64, 64, 3)\n",
        "\n",
        "# neural net parameters\n",
        "nn_params = {}\n",
        "nn_params['input_shape']       = image_shape\n",
        "nn_params['output_neurons']    = 4\n",
        "nn_params['loss']              = 'categorical_crossentropy'\n",
        "nn_params['output_activation'] = 'softmax'\n",
        "\n",
        "### Download data\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Driver%20Distraction%20Detection/metadata.csv'\n",
        "!wget -q --show-progress 'https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/Project%20-%20Driver%20Distraction%20Detection/image_data.npy'\n",
        "\n",
        "### pre-loading all data of interest\n",
        "_all_data = np.load('image_data.npy')\n",
        "_metadata = pkg.get_metadata(metadata_path, ['train','test','field'])\n",
        "\n",
        "### preparing definitions\n",
        "# downloading and loading data\n",
        "get_data_split = pkg.get_data_split\n",
        "get_metadata    = lambda :                 pkg.get_metadata(metadata_path, ['train','test'])\n",
        "get_train_data  = lambda flatten = False : pkg.get_train_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_test_data   = lambda flatten = False : pkg.get_test_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "get_field_data  = lambda flatten = False : pkg.get_field_data(flatten = flatten, all_data = _all_data, metadata = _metadata, image_shape = image_shape)\n",
        "\n",
        "# plotting\n",
        "plot_image = lambda data, num_ims,figsize=(8,6), labels = [], index = None: helpers.plot_image(data = data, num_ims=num_ims, figsize=figsize,labels = labels, index = index, image_shape = image_shape);\n",
        "plot_acc       = lambda history: helpers.plot_acc(history)\n",
        "\n",
        "# checking student model specs\n",
        "model_to_string        = lambda model: helpers.model_to_string(model)\n",
        "\n",
        "# models with input parameters\n",
        "TransferClassifier  = lambda name: models.TransferClassifier(name = name, nn_params = nn_params);\n",
        "\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_loss', verbose=0, save_best_only=True, save_weights_only=False, mode='auto', save_freq='epoch')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "metadata.csv        100%[===================>] 200.33K  --.-KB/s    in 0.006s  \n",
            "image_data.npy      100%[===================>] 423.61M  96.1MB/s    in 5.2s    \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFfMH3dYX_GM"
      },
      "source": [
        "# Milestone 1. Understanding and building Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SYqvCKWpKfRM"
      },
      "source": [
        "### What are neural networks?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qA1Rc_u3KoJT"
      },
      "source": [
        "Just as we went over last week, neural networks look something like this:\n",
        "![A 2 layer neural network](https://cdn-images-1.medium.com/max/1600/1*DW0Ccmj1hZ0OvSXi7Kz5MQ.jpeg)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3Q9S6SDcM8N9"
      },
      "source": [
        "Each orange and blue node is a neuron. The network itself is composed of a bunch of neurons that talk to each other and eventually give us a prediction.\n",
        "\n",
        "**In terms of this problem, what do each of the 4 blue neurons correspond to?**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0dik5yhBOERG"
      },
      "source": [
        "## Activity 1. Building networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E--8mjToZYBp"
      },
      "source": [
        "To build neural networks in Python, we use the packages known as `tensorflow` and `keras`. Let's learn how to build and use these networks!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G8PrEOTbhgNN"
      },
      "source": [
        "Tensorflow calls the various machine learning algorithms that it uses 'models'.  These 'models' are 'learning machines.''\n",
        "\n",
        "1. We **teach** models by **training** them on **data**.\n",
        "2. We **use** models to **predict** things.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqFAnQCxsgRm"
      },
      "source": [
        "# grab tools from our tensorflow and keras toolboxes!\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
        "from tensorflow.keras import optimizers"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPOqTta1sb6e"
      },
      "source": [
        "Before we train the model or use it to predict something, we have to **create** the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yus22AQpsqMH"
      },
      "source": [
        "# create our model by specifying and compiling it\n",
        "model_1 = Sequential()\n",
        "model_1.add(Dense(4, input_shape=(3,),activation = 'relu'))\n",
        "model_1.add(Dense(1, activation = 'linear'))\n",
        "model_1.compile(loss='mean_squared_error',\n",
        "                optimizer='adam',\n",
        "                metrics=['mean_squared_error'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LG3k7_983L1s"
      },
      "source": [
        "The things you'll want to pay most attention to as we go over how to build networks are:\n",
        "1. The number of neurons\n",
        "2. The activation of the neurons\n",
        "3. The losses and metrics\n",
        "\n",
        "Everything else will work with the default settings!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "781M4IyhssuA"
      },
      "source": [
        "Let's walk though what each of these lines of code means!\n",
        "\n",
        "**1. Specify model**\n",
        "\n",
        "```\n",
        "model = Sequential()\n",
        "```\n",
        "This line of code initializes our model! We build our network where the information flows from LEFT to RIGHT through the network in ONE DIRECTION as opposed to multiple directions. Neurons on the right never pass informations to neurons on the left of it.\n",
        "\n",
        "\n",
        "**2. Add layers to the network**\n",
        "```\n",
        "model.add(Dense(4,input_shape = (3,), activation = 'sigmoid'))\n",
        "```\n",
        "In this code, we use the `add` function on our model to add a `layer` of neurons to our network.\n",
        "\n",
        "The layer is DENSE, meaning every neuron of the current layer receives input from every neuron of the previous layer, **and** outputs information to every neuron of the next layer. This layer consists of `4` neurons. With our first `add` function, we also specify the `input_shape`, which consists of the `3` neurons in the input layer.\n",
        "\n",
        "We also specify what kind of output, or `activation` the neuron will give. If you want the neuron to output a number between 0 and 1 (like a probability!) you would use `'softmax'` or `'sigmoid'`. If you want the neuron to output any number, you can use `'linear'`! You'll also often see `'relu'`, which is when a neuron will only output positive numbers.\n",
        "\n",
        "```\n",
        "model.add(Dense(1, activation = 'linear'))\n",
        "```\n",
        "This code adds ANOTHER layer to the network that has 1 neuron. This one neuron is used to predict a continuous value!\n",
        "\n",
        "**3. Turn the model on by compiling it**\n",
        "\n",
        "After having built the network, we want to train and use it, so we have to 'turn it on' and 'compile' it. To turn it on, we have to specify at the very least, a loss, an optimizer, and some way of evaluating the model (metrics). Don't worry too much about what this means! Just know that this is necessary.\n",
        "\n",
        "```\n",
        "model.compile(loss='mean_squared_error',\n",
        "optimizer = 'adam',\n",
        "metrics = ['mean_squared_error'])\n",
        "  ```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "toYjQUOVtKDT"
      },
      "source": [
        "Once we've created our network, we can use it very simply! Just like we did with sklearn, we define our input data `x`, the true predictions from that data `y`, and then train our model with `fit`.\n",
        "\n",
        "```\n",
        "model.fit(x, y)\n",
        "```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-aozkfBxtWa7"
      },
      "source": [
        "To use the model, you can use it to predict something with:\n",
        "```\n",
        "y = model.predict_classes(x)\n",
        "```\n",
        "\n",
        "You can actually use the model before you even train it! It just won't perform very well."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wlnni4nPyCA3"
      },
      "source": [
        "## Exercise (Coding): A 2-Layer Model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xP5Z9cEMyBpM"
      },
      "source": [
        "\n",
        "We're going to build this model:\n",
        "\n",
        "![](http://cs231n.github.io/assets/nn1/neural_net.jpeg)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jxN_eHSoyBcF"
      },
      "source": [
        "This network can be described as:\n",
        "* Input Layer: 3\n",
        "* Layer 1 (Hidden): 4 neurons that are activated by `'relu'`\n",
        "* Layer 2 (Output): 2 neurons that are activated by `'softmax'`\n",
        "\n",
        "\n",
        "We also want to compile the model with\n",
        "`loss = 'categorical_crossentropy'`"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace the None values with your group!\n",
        "model_2 = Sequential()\n",
        "model_2.add(Dense(4, input_shape = (3,), activation = \"relu\")) #input shape = input layer.\n",
        "model_2.add(Dense(2, activation = \"softmax\"))\n",
        "\n",
        "model_2.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])\n"
      ],
      "metadata": {
        "id": "uhKNoS95BmDV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IH2UGOK4vuZ4",
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c6dcc32f-4833-4bb7-efc8-f075daf98f20"
      },
      "source": [
        "#@title Run this to test if your model is right!\n",
        "model_2_answer = Sequential()\n",
        "model_2_answer.add(Dense(4, input_shape = (3,), activation = 'relu'))\n",
        "model_2_answer.add(Dense(2, activation = 'softmax'))\n",
        "model_2_answer.compile(loss='categorical_crossentropy',\n",
        "                       optimizer = 'adam',\n",
        "                       metrics = ['accuracy'])\n",
        "\n",
        "if model_to_string(model_2) == model_to_string(model_2_answer):\n",
        "  print('Good job! Your model worked')\n",
        "else:\n",
        "  print('Please check your code again!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job! Your model worked\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tyLV1oHjT62K"
      },
      "source": [
        "# Milestone 2. Applying Neural Networks to Detecting Distracted Drivers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hBp4yqqoJF65"
      },
      "source": [
        "## Instructor-Led Discussion: Model Architecture"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PD3Z0QamJF68"
      },
      "source": [
        "\n",
        "In our problem, we are given `images` of shape `(64,64,3)`, each assigned to one of 4 labels: `Attentive`, `DrinkingCoffee`, `UsingRadio`, or `UsingMirror`. We want to identify the key things that we need to design our network.\n",
        "\n",
        "In your group, discuss:\n",
        "\n",
        "* What are our inputs?\n",
        "* What is/are our outputs?\n",
        "\n",
        "How could this look in a neural network diagram?\n",
        "\n",
        "**Let's discuss as a class!!**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K-7tm6ZlJF7G"
      },
      "source": [
        "## Activity 2b. Building our custom neural network"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8pozxKuJF7I"
      },
      "source": [
        "### Key Points"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UN4Pyw4qJF7N"
      },
      "source": [
        "We will build a simple 2-layer network, also known as a perceptron, for our first model!\n",
        "\n",
        "\n",
        "For our model, we have as our layers:\n",
        "* Input Layer:  However many inputs there are!\n",
        "* Layer 1 (Hidden): 128 neurons that are activated by `'relu'`\n",
        "* Layer 2 (Output): 4 neurons (1 per possible predicted class) that should have an appropriate activation.\n",
        "* We will compile with the `optimizers.SGD(learning_rate=1e-4, momentum=0.95)` optimizer\n",
        "\n",
        "As a hint for the output activation and the compilation loss, we know that:\n",
        "* **Binary classification** problems require an output activation of `'sigmoid'` and a loss of `'binary_cross_entropy'`\n",
        "* **Multi-class classification** problems require an output activation of `'softmax'` and a loss of `'categorical_crossentropy'`\n",
        "* **Linear regression** problems require an output activation of `'linear'` and a loss of `'mean_squared_error'`\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LzFTOYMiJF7Q"
      },
      "source": [
        "###Build Your Model\n",
        "Remember that a multi-layer perceptron requires flattened input, but our images have shape `(64, 64, 3)`.\n",
        "We have added the `Flatten()` layer for you, which flattens our input as desired -- your job is to add the rest of the layers and compile your model!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZJUSGm_oJF7h"
      },
      "source": [
        "model_3 = Sequential()\n",
        "model_3.add(Flatten(input_shape = (64, 64, 3)))\n",
        "model_3.add(Dense(128, input_shape = (3,), activation = \"relu\")) #input shape = input layer.\n",
        "model_3.add(Dense(4, activation = \"softmax\"))\n",
        "\n",
        "model_3.compile(loss='categorical_crossentropy',\n",
        "                optimizer = 'adam',\n",
        "                metrics = ['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9i-xfhnaJF7q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca838576-0afb-4aaa-b942-776405fc727d"
      },
      "source": [
        "#@title Run this to test if your model is right! { display-mode: \"form\" }\n",
        "model_3_answer = Sequential()\n",
        "model_3_answer.add(Flatten(input_shape = (64, 64, 3)))\n",
        "model_3_answer.add(Dense(units = 128, activation = 'relu'))\n",
        "model_3_answer.add(Dense(units = 4, activation = 'softmax'))\n",
        "\n",
        "model_3_answer.compile(loss='categorical_crossentropy',\n",
        "              optimizer=optimizers.SGD(learning_rate=1e-10, momentum=0.95),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "if model_to_string(model_3) == model_to_string(model_3_answer):\n",
        "  print('Good job, you specified it correctly!')\n",
        "else:\n",
        "  print('Please check your code again!')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Good job, you specified it correctly!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nayBlbHmj4Ii"
      },
      "source": [
        "### Exercise (Coding)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AItvQJE5NY0H"
      },
      "source": [
        "## Now let's apply data augmentation to our images.\n",
        "\n",
        "After defining our model, we often deal with the issue of a limited amount of training data in machine learning. As you might imagine, a model trained on a small set of data is likely to perform poorly because of overfitting, which means it doesn't generalize well to new, unseen data.\n",
        "\n",
        "This is where data augmentation steps in. By creating modifications of our training data, we can artificially increase its size and diversity, improving the model's performance and generalization ability.\n",
        "\n",
        "In the case of images, we can use the `ImageDataGenerator` class from Keras. This allows us to make random transformations to our images, such as rotations, translations, zooms, and flips, so that our model can learn from a wider range of data.\n",
        "\n",
        "Here's how we use it:\n",
        "\n",
        "```python\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=10,\n",
        "    width_shift_range=0.1,\n",
        "    height_shift_range=0.1,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")\n",
        "```\n",
        "With this setup:\n",
        "- `rotation_range=10`: our image will be randomly rotated between 0 and 10 degrees.\n",
        "- `width_shift_range=0.1` and `height_shift_range=0.1`: our image will be moved up to 10% horizontally and vertically.\n",
        "- `shear_range=0.2`: a shear (stretching) transformation will be randomly applied up to 20%.\n",
        "- `zoom_range=0.2`: our image will be randomly zoomed up to 20%.\n",
        "- `horizontal_flip=True`: our image can be flipped horizontally.\n",
        "- `fill_mode='nearest'`: any new pixels created by these transformations will be filled with the 'nearest' color in the original image.\n",
        "\n",
        "**It's time to use the augmented data to train our model!**\n",
        "\n",
        "## Let's now train our perceptron on images from the train data!\n",
        "\n",
        "Unlike the models that we used in sklearn, our neural networks are pretty finnicky. Their performance depends a lot on *how much* they train. As we'll see, they usually get better with more training BUT actually can get worse with too much training. With too much training, our model can memorize the training data (overfitting), and so doesn't actually think (generalize) when it is tested.\n",
        "\n",
        "The extra options in our `fit()` function pertain to how the neural networks train. Don't worry too much about the extra options, what really matters for us is that the right data is specified.\n",
        "\n",
        "To use `fit`, we use the following code:\n",
        "```\n",
        "X_train, y_train = get_train_data()\n",
        "history = our_model.fit(data_augmentation.flow(X_train, y_train, batch_size=32), epochs = 10, validation_data = (X_test, y_test), shuffle = True, callbacks = [monitor])\n",
        "```\n",
        "What are all these options?\n",
        "* `epochs`: how many times the model trains on the entire data set\n",
        "* `shuffle`: mixes the training dataset so the model pays better attention to the data and learns better while training\n",
        "* `validation_data`: we request that our model tests itself on the `test_data` after every epoch. Since our model is finnicky, instead of testing our model at the end of the training, we test it throughout.\n",
        "\n",
        " `history` gives us a data structure which allows us to plot the training and validation accuracy over time.\n",
        "\n",
        "We have one more option too:\n",
        "* `callbacks`: With a custom command, we tell our model to save the best version of itself to a model file called `model.h5`.\n",
        "\n",
        "\n",
        "**Specifically, load in the training and testing data and then train your MLP model.**\n",
        "\n",
        "_Note: Remember you already created your model above (model_3)._\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-tjgTdXv6Kh7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5bd1d1c8-2ee8-4adb-fd69-1b57873be1c5"
      },
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define our monitor. Don't worry about the parameters here except for './model.h5',\n",
        "# which is the file that our model saves to.\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_loss', verbose=0,\n",
        "                          save_best_only=True, save_weights_only=False,\n",
        "                          mode='auto', save_freq='epoch')\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=10,  # Rotate images randomly up to 10 degrees\n",
        "    width_shift_range=0.1,  # Shift images horizontally by a fraction of the width\n",
        "    height_shift_range=0.1,  # Shift images vertically by a fraction of the height\n",
        "    shear_range=0.2,  # Apply shear transformation with a shear angle up to 20 degrees\n",
        "    zoom_range=0.2,  # Randomly zoom images by a factor up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    fill_mode='nearest'  # Fill in newly created pixels after rotation or shifting\n",
        ")\n",
        "\n",
        "### YOUR CODE HERE (get the train data and test data!)\n",
        "(X_train, y_train) = get_train_data()\n",
        "(X_test, y_test) = get_test_data()\n",
        "\n",
        "### END CODE\n",
        "\n",
        "# Reshape the data\n",
        "X_train = X_train.reshape([-1, 64, 64, 3])\n",
        "X_test = X_test.reshape([-1, 64, 64, 3])\n",
        "\n",
        "# Convert string labels into numpy arrays.\n",
        "y_train = label_to_numpy(y_train)\n",
        "y_test = label_to_numpy(y_test)\n",
        "\n",
        "### YOUR CODE HERE (fit your model!)\n",
        "history = model_3.fit(data_augmentation.flow(X_train, y_train, batch_size=32), epochs = 10, validation_data = (X_test, y_test), shuffle = True, callbacks = [monitor])\n",
        "\n",
        "### END CODE\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "211/211 [==============================] - 23s 107ms/step - loss: 1.5660 - accuracy: 0.3599 - val_loss: 1.6760 - val_accuracy: 0.3065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 2/10\n",
            "211/211 [==============================] - 13s 61ms/step - loss: 1.2146 - accuracy: 0.4535 - val_loss: 1.4235 - val_accuracy: 0.3370\n",
            "Epoch 3/10\n",
            "211/211 [==============================] - 14s 67ms/step - loss: 1.1605 - accuracy: 0.4847 - val_loss: 1.3234 - val_accuracy: 0.4304\n",
            "Epoch 4/10\n",
            "211/211 [==============================] - 13s 62ms/step - loss: 1.1300 - accuracy: 0.4951 - val_loss: 1.1800 - val_accuracy: 0.4565\n",
            "Epoch 5/10\n",
            "211/211 [==============================] - 13s 62ms/step - loss: 1.1044 - accuracy: 0.5036 - val_loss: 1.2687 - val_accuracy: 0.4511\n",
            "Epoch 6/10\n",
            "211/211 [==============================] - 13s 61ms/step - loss: 1.0932 - accuracy: 0.5150 - val_loss: 1.1680 - val_accuracy: 0.4543\n",
            "Epoch 7/10\n",
            "211/211 [==============================] - 13s 61ms/step - loss: 1.0527 - accuracy: 0.5409 - val_loss: 1.2956 - val_accuracy: 0.4815\n",
            "Epoch 8/10\n",
            "211/211 [==============================] - 15s 69ms/step - loss: 1.0378 - accuracy: 0.5379 - val_loss: 1.0943 - val_accuracy: 0.5761\n",
            "Epoch 9/10\n",
            "211/211 [==============================] - 13s 61ms/step - loss: 1.0096 - accuracy: 0.5583 - val_loss: 1.0025 - val_accuracy: 0.5217\n",
            "Epoch 10/10\n",
            "211/211 [==============================] - 13s 61ms/step - loss: 1.0246 - accuracy: 0.5418 - val_loss: 1.0112 - val_accuracy: 0.5837\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttqZa25BVDeR"
      },
      "source": [
        "As our model trained, it told us a few things. The most important things to us are:\n",
        "* how accurate it was when training on the training set (reported as `acc`)\n",
        "* how accurate it was on the test set (reported as `val_acc`)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rsOkqi035XRE"
      },
      "source": [
        "We can actually plot how how well our model did across epochs using the model's `history`!\n",
        "To do this, we call:\n",
        "```\n",
        "plot_acc(history)\n",
        "```\n",
        "\n",
        "Try `plot_acc` below!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xvhyL-jJ9rKt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 455
        },
        "outputId": "2313a71e-1213-4026-de1c-c1c1fda2a22d"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "plot_acc(history)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uofnvxpz-qdI"
      },
      "source": [
        "## Instructor-Led Discussion\n",
        "\n",
        "Is this a good model? Does your model overfit? How do you know?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g9CmrRkgT5ZS"
      },
      "source": [
        "# Milestone 3. Exploring Convolutional Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RehfGJJWZFeG"
      },
      "source": [
        "So, how is a convolutional neural network specified in tensorflow/keras? Let's walk through this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "orX1T8VVqYo1"
      },
      "source": [
        "### Exercise (Coding)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LMi7nEjaIeN4"
      },
      "source": [
        "Our convolutional neural network is specified via:\n",
        "\n",
        "```\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(64, (3, 3), input_shape=(__, __, __)))\n",
        "cnn.add(Activation('relu'))\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "cnn.add(Dense(units = 128, activation = 'relu'))\n",
        "cnn.add(Dense(units = NUM_OUTPUTS, activation = 'softmax'))\n",
        "```\n",
        "\n",
        "And compiled with:\n",
        "\n",
        "```\n",
        "cnn.compile(loss=__, optimizer=__, metrics=__)\n",
        "```\n",
        "\n",
        "We see that we have 1 convolution layer that takes in our inputs, and then 2 dense layers. Overall this is a 3 layer network.\n",
        "\n",
        "After specifying the network, we can compile it and train it just like before! Note:\n",
        "* we want our `loss` to be `'categorical_crossentropy'`\n",
        "* our `optimizer` will be  `optimizers.SGD(learning_rate=1e-3, momentum=0.95)`\n",
        "* our `metrics` are `['accuracy']`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vNDBeK9K2ZzU"
      },
      "source": [
        "### YOUR CODE HERE - build and compile your cnn (hint: see above and fill in the blanks!)\n",
        "\n",
        "# specify the network\n",
        "cnn = Sequential()\n",
        "cnn.add(Conv2D(64, (3, 3), input_shape=(64, 64, 3)))\n",
        "cnn.add(Dense(units = 128, activation = 'relu'))\n",
        "cnn.add(Activation('relu'))\n",
        "\n",
        "cnn.add(MaxPooling2D(pool_size=(2, 2)))\n",
        "cnn.add(Flatten())\n",
        "\n",
        "cnn.add(Dense(units = 4, activation = 'softmax'))\n",
        "\n",
        "\n",
        "# compile the network\n",
        "cnn.compile(loss=\"categorical_crossentropy\", optimizer=optimizers.SGD(learning_rate=1e-3, momentum=0.95), metrics=['accuracy'])\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s858q9Nd4vk-"
      },
      "source": [
        "Once we've compiled the network, train it for about 50 epochs (Or less, it's up to you!).\n",
        "Remember how you did this for the MLP (`model_2`)? Also, don't forget to use data augmentation!\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_HTn3r594KWZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "021e1bc3-aeb2-4e82-ed8e-f716fc742e01"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "\n",
        "# Define our monitor. Don't worry about the parameters here except for './model.h5',\n",
        "# which is the file that our model saves to.\n",
        "monitor = ModelCheckpoint('./model.h5', monitor='val_loss', verbose=0,\n",
        "                          save_best_only=True, save_weights_only=False,\n",
        "                          mode='auto', save_freq='epoch')\n",
        "\n",
        "# Create an instance of ImageDataGenerator for data augmentation\n",
        "data_augmentation = ImageDataGenerator(\n",
        "    rotation_range=10,  # Rotate images randomly up to 10 degrees\n",
        "    width_shift_range=0.1,  # Shift images horizontally by a fraction of the width\n",
        "    height_shift_range=0.1,  # Shift images vertically by a fraction of the height\n",
        "    shear_range=0.2,  # Apply shear transformation with a shear angle up to 20 degrees\n",
        "    zoom_range=0.2,  # Randomly zoom images by a factor up to 20%\n",
        "    horizontal_flip=True,  # Flip images horizontally\n",
        "    fill_mode='nearest'  # Fill in newly created pixels after rotation or shifting\n",
        ")\n",
        "\n",
        "### YOUR CODE HERE (get the train data and test data!)\n",
        "(X_train, y_train) = get_train_data()\n",
        "(X_test, y_test) = get_test_data()\n",
        "\n",
        "### END CODE\n",
        "\n",
        "# Reshape the data\n",
        "X_train = X_train.reshape([-1, 64, 64, 3])\n",
        "X_test = X_test.reshape([-1, 64, 64, 3])\n",
        "\n",
        "# Convert string labels into numpy arrays.\n",
        "y_train = label_to_numpy(y_train)\n",
        "y_test = label_to_numpy(y_test)\n",
        "\n",
        "### YOUR CODE HERE (fit your model!)\n",
        "history = cnn.fit(data_augmentation.flow(X_train, y_train, batch_size=100), epochs = 50, validation_data = (X_test, y_test), shuffle = True, callbacks = [monitor])\n",
        "\n",
        "### END CODE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "68/68 [==============================] - 109s 2s/step - loss: 1.3543 - accuracy: 0.3330 - val_loss: 1.3556 - val_accuracy: 0.4076\n",
            "Epoch 2/50\n"
          ]
        },
        {
          "metadata": {
            "tags": null
          },
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/engine/training.py:3103: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
            "  saving_api.save_model(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "68/68 [==============================] - 107s 2s/step - loss: 1.2589 - accuracy: 0.4210 - val_loss: 1.3560 - val_accuracy: 0.4272\n",
            "Epoch 3/50\n",
            "68/68 [==============================] - 112s 2s/step - loss: 1.1542 - accuracy: 0.5007 - val_loss: 1.3746 - val_accuracy: 0.3663\n",
            "Epoch 4/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 1.0989 - accuracy: 0.5396 - val_loss: 1.2883 - val_accuracy: 0.5272\n",
            "Epoch 5/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 1.0362 - accuracy: 0.5738 - val_loss: 1.2139 - val_accuracy: 0.5935\n",
            "Epoch 6/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.9901 - accuracy: 0.6034 - val_loss: 1.3689 - val_accuracy: 0.4935\n",
            "Epoch 7/50\n",
            "68/68 [==============================] - 105s 2s/step - loss: 0.9542 - accuracy: 0.6295 - val_loss: 1.3401 - val_accuracy: 0.5924\n",
            "Epoch 8/50\n",
            "68/68 [==============================] - 116s 2s/step - loss: 0.9154 - accuracy: 0.6539 - val_loss: 1.2748 - val_accuracy: 0.5880\n",
            "Epoch 9/50\n",
            "68/68 [==============================] - 108s 2s/step - loss: 0.8792 - accuracy: 0.6666 - val_loss: 1.4425 - val_accuracy: 0.5130\n",
            "Epoch 10/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.8295 - accuracy: 0.6947 - val_loss: 1.4076 - val_accuracy: 0.5402\n",
            "Epoch 11/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.8013 - accuracy: 0.7026 - val_loss: 1.3209 - val_accuracy: 0.5641\n",
            "Epoch 12/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.7648 - accuracy: 0.7219 - val_loss: 1.2562 - val_accuracy: 0.6348\n",
            "Epoch 13/50\n",
            "68/68 [==============================] - 105s 2s/step - loss: 0.7587 - accuracy: 0.7225 - val_loss: 1.3724 - val_accuracy: 0.4989\n",
            "Epoch 14/50\n",
            "68/68 [==============================] - 123s 2s/step - loss: 0.7286 - accuracy: 0.7243 - val_loss: 1.3405 - val_accuracy: 0.5435\n",
            "Epoch 15/50\n",
            "68/68 [==============================] - 131s 2s/step - loss: 0.7039 - accuracy: 0.7455 - val_loss: 1.3300 - val_accuracy: 0.6315\n",
            "Epoch 16/50\n",
            "68/68 [==============================] - 113s 2s/step - loss: 0.6612 - accuracy: 0.7571 - val_loss: 1.3412 - val_accuracy: 0.5087\n",
            "Epoch 17/50\n",
            "68/68 [==============================] - 110s 2s/step - loss: 0.6749 - accuracy: 0.7552 - val_loss: 1.1998 - val_accuracy: 0.6076\n",
            "Epoch 18/50\n",
            "68/68 [==============================] - 108s 2s/step - loss: 0.6462 - accuracy: 0.7661 - val_loss: 1.2773 - val_accuracy: 0.5685\n",
            "Epoch 19/50\n",
            "68/68 [==============================] - 123s 2s/step - loss: 0.6238 - accuracy: 0.7762 - val_loss: 1.4622 - val_accuracy: 0.6293\n",
            "Epoch 20/50\n",
            "68/68 [==============================] - 116s 2s/step - loss: 0.6219 - accuracy: 0.7780 - val_loss: 1.4070 - val_accuracy: 0.5239\n",
            "Epoch 21/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.6067 - accuracy: 0.7823 - val_loss: 1.3968 - val_accuracy: 0.5967\n",
            "Epoch 22/50\n",
            "68/68 [==============================] - 103s 2s/step - loss: 0.6051 - accuracy: 0.7830 - val_loss: 1.4169 - val_accuracy: 0.5467\n",
            "Epoch 23/50\n",
            "68/68 [==============================] - 104s 2s/step - loss: 0.5816 - accuracy: 0.7896 - val_loss: 1.5696 - val_accuracy: 0.5359\n",
            "Epoch 24/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.5949 - accuracy: 0.7851 - val_loss: 1.4135 - val_accuracy: 0.5772\n",
            "Epoch 25/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.5636 - accuracy: 0.7958 - val_loss: 1.4301 - val_accuracy: 0.5652\n",
            "Epoch 26/50\n",
            "68/68 [==============================] - 103s 2s/step - loss: 0.5464 - accuracy: 0.8038 - val_loss: 1.5469 - val_accuracy: 0.5380\n",
            "Epoch 27/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.5584 - accuracy: 0.7965 - val_loss: 1.4820 - val_accuracy: 0.5413\n",
            "Epoch 28/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.5325 - accuracy: 0.8129 - val_loss: 1.3527 - val_accuracy: 0.5772\n",
            "Epoch 29/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.5472 - accuracy: 0.8023 - val_loss: 1.5775 - val_accuracy: 0.5587\n",
            "Epoch 30/50\n",
            "68/68 [==============================] - 115s 2s/step - loss: 0.5305 - accuracy: 0.8122 - val_loss: 1.4720 - val_accuracy: 0.5707\n",
            "Epoch 31/50\n",
            "68/68 [==============================] - 112s 2s/step - loss: 0.5168 - accuracy: 0.8181 - val_loss: 1.4881 - val_accuracy: 0.5859\n",
            "Epoch 32/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.5216 - accuracy: 0.8137 - val_loss: 1.4703 - val_accuracy: 0.5946\n",
            "Epoch 33/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.4899 - accuracy: 0.8247 - val_loss: 1.4603 - val_accuracy: 0.5880\n",
            "Epoch 34/50\n",
            "68/68 [==============================] - 105s 2s/step - loss: 0.4810 - accuracy: 0.8318 - val_loss: 1.5183 - val_accuracy: 0.5761\n",
            "Epoch 35/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4866 - accuracy: 0.8230 - val_loss: 1.4737 - val_accuracy: 0.5739\n",
            "Epoch 36/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4764 - accuracy: 0.8294 - val_loss: 1.4064 - val_accuracy: 0.5967\n",
            "Epoch 37/50\n",
            "68/68 [==============================] - 104s 2s/step - loss: 0.4863 - accuracy: 0.8264 - val_loss: 1.5858 - val_accuracy: 0.5793\n",
            "Epoch 38/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4812 - accuracy: 0.8333 - val_loss: 1.4802 - val_accuracy: 0.5793\n",
            "Epoch 39/50\n",
            "68/68 [==============================] - 108s 2s/step - loss: 0.4963 - accuracy: 0.8198 - val_loss: 1.5089 - val_accuracy: 0.5739\n",
            "Epoch 40/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4870 - accuracy: 0.8294 - val_loss: 1.3756 - val_accuracy: 0.6033\n",
            "Epoch 41/50\n",
            "68/68 [==============================] - 108s 2s/step - loss: 0.4625 - accuracy: 0.8354 - val_loss: 1.4622 - val_accuracy: 0.6065\n",
            "Epoch 42/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.4455 - accuracy: 0.8456 - val_loss: 1.5275 - val_accuracy: 0.5826\n",
            "Epoch 43/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.4380 - accuracy: 0.8493 - val_loss: 1.6093 - val_accuracy: 0.5652\n",
            "Epoch 44/50\n",
            "68/68 [==============================] - 108s 2s/step - loss: 0.4331 - accuracy: 0.8464 - val_loss: 1.7255 - val_accuracy: 0.5826\n",
            "Epoch 45/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4488 - accuracy: 0.8404 - val_loss: 1.5422 - val_accuracy: 0.5848\n",
            "Epoch 46/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4449 - accuracy: 0.8410 - val_loss: 1.6163 - val_accuracy: 0.5924\n",
            "Epoch 47/50\n",
            "68/68 [==============================] - 103s 2s/step - loss: 0.4363 - accuracy: 0.8450 - val_loss: 1.7287 - val_accuracy: 0.5489\n",
            "Epoch 48/50\n",
            "68/68 [==============================] - 106s 2s/step - loss: 0.4237 - accuracy: 0.8514 - val_loss: 1.6710 - val_accuracy: 0.5978\n",
            "Epoch 49/50\n",
            "68/68 [==============================] - 107s 2s/step - loss: 0.4188 - accuracy: 0.8513 - val_loss: 1.5740 - val_accuracy: 0.5880\n",
            "Epoch 50/50\n",
            "68/68 [==============================] - 105s 2s/step - loss: 0.4188 - accuracy: 0.8543 - val_loss: 1.6371 - val_accuracy: 0.5717\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ic7tsMlm4whB"
      },
      "source": [
        "And see how well it did! Let's visualize the training and validation accuracy again."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7jZAgQbM4nmg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "f676a864-737a-4d38-ed1d-f5274ec1e0ad"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "plot_acc(history)\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'plot_acc' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-39086ef5e958>\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m### YOUR CODE HERE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplot_acc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m### END CODE\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'plot_acc' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A7R8Mlgzl8wY"
      },
      "source": [
        "**Nice training accuracy! Does the model still overfit?**\n",
        "\n",
        "*(Optional:)* Re-initialize your CNN and train it on a smaller number of epochs. What do you observe?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5scKiYAYE8g"
      },
      "source": [
        "\n",
        "# Milestone 4. Expert models: Transfer learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "laXN17IK23GY"
      },
      "source": [
        "So far, we've used models that were built from 'scratch'. Unfortunately, our training data is small relative to the amount of data available in the real world, so just training on our dataset is going to be inherently limited.\n",
        "\n",
        "Luckily, there are **expert models**, or state-of-the-art models that have been trained by the world's top researchers! While these expert models haven't trained on our training data, they have trained extensively on larger datasets. We can input our data and reasonably expect that they will pick up our task fairly quickly.\n",
        "\n",
        "In deep learning, the idea of using a model trained on another task as a starting point for your model is known as **transfer learning**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FybhlxdVYFbv"
      },
      "source": [
        "## Activity 4a. Transfer Learning for Distracted Driving\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DChmzlt3ARPy"
      },
      "source": [
        "### Instructor-Led Discussion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lFtHOYI2AdSs"
      },
      "source": [
        "For our transfer learning, we're going to use expert models built upon the famous 'ImageNet' classification problem.\n",
        "\n",
        "In ImageNet, participants were challenged to build machine learning models that could distinguish 14 million images' categories, where there were > 20,000 categories available.\n",
        "\n",
        "Below, we see examples of 3 different categories.\n",
        "\n",
        "![](https://storage.googleapis.com/inspirit-ai-data-bucket-1/Data/AI%20Scholars/Sessions%206%20-%2010%20(Projects)/ImageNet.jpg)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J-E_AiG-CFj0"
      },
      "source": [
        "One of the experts we can use is VGG16. VGG16 is a specific convolutional neural network that was allowed to study the 14 million images 74 times. *(Read more about VGG16 [here!](https://neurohive.io/en/popular-networks/vgg16/))*\n",
        "\n",
        "After training, VGG16 was able to guess something close to the real label (top-5 accuracy) better than a human can."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IvkajtdHAbzL"
      },
      "source": [
        "![](https://cdn-images-1.medium.com/max/1600/0*V1muWIDnPVwZUuEv.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vwj8o5X3D325"
      },
      "source": [
        "We're going to take an expert model like VGG16 and let it train on OUR images. Hopefully, its experience training on those 14 million images will help it understand distracted driving!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g-357WWC7qJJ"
      },
      "source": [
        "### Exercise (Coding) | Within a student group"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uz_mVsECHvro"
      },
      "source": [
        "Let's tap an expert model to help us out with our distracted driving prediction!\n",
        "\n",
        "We provide a wrapper that lets you 'call' up and employ expert models. You can call it like...\n",
        "\n",
        "```\n",
        "transfer = TransferClassifier(name = 'VGG16')\n",
        "```\n",
        "\n",
        "The wrapper will also add a few new layers to the model that can learn specifics about **our task**.\n",
        "\n",
        "\n",
        "The experts we have on hand are:\n",
        "* `VGG16`\n",
        "* `VGG19`\n",
        "* `ResNet50`\n",
        "* `DenseNet121`\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9ye0v5CrJBvs"
      },
      "source": [
        "Afterwards, see if you can get 85% accuracy with your model! Try it with and without data augmentation and see how different it preforms."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0VB79BCx7tvg"
      },
      "source": [
        "# As always, we get our data first\n",
        "(X_train, y_train) = get_train_data(flatten=True)\n",
        "(X_test, y_test) = get_test_data(flatten=True)\n",
        "\n",
        "X_train = X_train.reshape([-1, 64, 64, 3])\n",
        "X_test = X_test.reshape([-1, 64, 64, 3])\n",
        "\n",
        "y_train = label_to_numpy(y_train)\n",
        "y_test = label_to_numpy(y_test)\n",
        "\n",
        "### YOUR CODE HERE\n",
        "\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "buizA9EUYG5_"
      },
      "source": [
        "\n",
        "## (Optional) Activity 4b. Transfer learning in tensorflow/keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BGcLFbq28GA6"
      },
      "source": [
        "If you want to see how to implement transfer learning in tensorflow/keras, you can try this exercise!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B5hq6cYOYHq_"
      },
      "source": [
        "\n",
        "### Exercise (Coding) | Within a student group\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "euLIWetzEclz"
      },
      "source": [
        "First, let's import VGG16's architecture from `tensorflow.keras`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bLIe66rQJTua"
      },
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cMTOXiLXE0OX"
      },
      "source": [
        "Let's now load up VGG16. We only want the convolutional layers of the model - that is, the layers that are most responsible for giving the model its visual understanding. The 'Dense/Fully Connected (FC)' layers are thought to be more specific to the ImageNet challenge."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fvIzSJ6_FONj"
      },
      "source": [
        "# Load the VGG architecture but do not include the FC layers\n",
        "vgg_expert = VGG16(weights = 'imagenet', include_top = False, input_shape = (64, 64, 3))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5XyVd6o6FsRX"
      },
      "source": [
        "Now, we're going to plug the VGG into a custom model. to do this, we do the following:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EMzqTaE7F25Q"
      },
      "source": [
        "# We add the first 12 layers of VGG to our own model: vgg_model\n",
        "vgg_model = Sequential()\n",
        "vgg_model.add(vgg_expert)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5oAm86oF63h"
      },
      "source": [
        "We want to add custom layers to our model... specifically,\n",
        "* `GlobalAveragePooling2D() # helps our vgg expert`\n",
        "* `Dense(1024, activation = 'relu') # we've seen dense before!`\n",
        "* `Dropout(0.3) # we've experimented with dropout before!`\n",
        "* `Dense(512, activation = 'relu')`\n",
        "* `Dropout(0.3)`\n",
        "* `Dense(4, activation = 'softmax') # our output layer!`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL6rdBDsGtOe"
      },
      "source": [
        "### Instructor-Led Discussion: Why do we add these layers to the end of the network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VpDt946zGhCZ"
      },
      "source": [
        "# Then we add our own layers on top of our vgg_model\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZJrK16jvGlMK"
      },
      "source": [
        "And finally compile it with\n",
        "* loss: `categorical_crossentropy`\n",
        "* optimizer: `optimizers.SGD(learning_rate = 1e-4, momentum = 0.95)`\n",
        "* metrics: `accuracy`\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8SDKzK8eGxIM"
      },
      "source": [
        "# Compile our model\n",
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eydWywLiG53c"
      },
      "source": [
        "Finally, train your model on the `training data`.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRpKBNUwHqEH"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mkrw6963HI1W"
      },
      "source": [
        "Use `plot_acc()` to visualize how your model did!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-gpwNg62GGJs"
      },
      "source": [
        "### YOUR CODE HERE\n",
        "\n",
        "### END CODE"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fnyfhtyotUH0"
      },
      "source": [
        "## Nice! Hopefully your transfer learning model did much better on the validation data set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14rKID6MOo4N"
      },
      "source": [
        "![](http://static1.squarespace.com/static/56ccc8724c2f8548059fbcfe/58f6daea15d5dbcc64ef63aa/5cae2d770d92977242838baa/1557942174418/SW-DistractedDriving-Clean-1.jpg?format=1500w)"
      ]
    }
  ]
}